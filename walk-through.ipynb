{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Walk-Through\n",
    "\n",
    "Often the data we want isn't available in a downloadable format or an web API. But if we can see the information we want in the content of web pages, we can usually scrape it.\n",
    "\n",
    "In this walk-through, we will:\n",
    "\n",
    "1. Discuss fundamental web technologies relevant to web scraping\n",
    "2. Introduce third-party Python packages that support the web scraping workflow\n",
    "3. Write some code that scrapes data from the Missouri State Highway Patrol's [website](https://www.mshp.dps.missouri.gov/HP68/SearchAction)\n",
    "\n",
    "## Don't do this unless you really need to\n",
    "\n",
    "Scraping a website should be your last resort. Unless under extraordinary circumstances, you should strive to work within the usual guidelines of public information requests.\n",
    "\n",
    "Talk to the people who run the website you want to scrape, and ask them to provide you with their data, which is most likely in a relational database system (e.g., SQL Server, MySQL, Oracle). Be patient, kind and respectful.\n",
    "\n",
    "## How the web works\n",
    "\n",
    "When your web browser loads a website and you point and click your way around the pages of that site, there are several technologies working behind the scenes that make all this possible. So in order to do web scraping, it's helpful to have a good idea of [how the web works](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/How_the_Web_works). \n",
    "\n",
    "In particular, when we scrape a website, we interact directly with (at the very least) two fundamental web technologies.\n",
    "\n",
    "### HTTP\n",
    "\n",
    "[HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP) stands for **H**yper**t**ext **T**ransfer **P**rotocol, and it is the foundation of any data exchange on Web. It sets the rules for how clients like your preferred web browser (e.g., Firefox, Safari, Chrome) communicate with web servers that host web pages and other resources you might fetch.\n",
    "\n",
    "The HTTP flow starts when a client sends a request to server. All requests have the following parts:\n",
    "\n",
    "1. A [method](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) indicating the user's desired action.\n",
    "2. A path to a resource, indicated by Uniform Resource Locator (aka, [URL](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL)).\n",
    "3. Optional headers that convey additional information from clients to the server.\n",
    "\n",
    "### HTML\n",
    "\n",
    "The second fundamental web technology we interact with in web scraping is **H**yper**t**ext **M**arkup **L**anguage (aka, [HTML](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics)). It's isn't a programming language like Python or Javascript. Rather, it is a language for defining the structure of web pages. Every web page is a document written in HTML.\n",
    "\n",
    "In an HTML document, content is annotated (hence, the \"Markup\" part) using tags like [`<h1>` through `<h6>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements) for headings and [`<p>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p) for paragraphs.\n",
    "\n",
    "Tags, plus their optional attributes and the content that they wrap, form [elements](https://developer.mozilla.org/en-US/docs/Glossary/element), which are the atomic units of an HTML document.\n",
    "\n",
    "![](https://media.prod.mdn.mozit.cloud/attachments/2014/04/09/7659/a731e40efad1f6e0b728bfcf86c0035b/anatomy-of-an-html-element.png)\n",
    "\n",
    "## How a web scraper works\n",
    "\n",
    "With these two technologies in mind, we have an essential grasp of the work a web scraper needs to do:\n",
    "\n",
    "1. Fetch HTML documents using HTTP\n",
    "2. Parse those resources into a machine readable structure\n",
    "3. Extract the exact content we want from that structure.\n",
    "4. Store that extracted content in a format that affords further reporting, analysis, etc.\n",
    "\n",
    "Often in the news room, reporters will perform this exact series of tasks using a web browser, a spreadsheet application and good ol' copy and paste. That tedious, repeative work *is* web scraping, though some might not use such a buzzy label to describe such a boring process.\n",
    "\n",
    "Web scraping, as it is typically understood, is simply the automation of this general workflow, and automating this process requires different tools and different (typically non-graphical) interfaces.\n",
    "\n",
    "## Essential Python packages\n",
    "\n",
    "A couple of third-party Python packages are widely used in web scraping to handle the two web technologies described above.\n",
    "\n",
    "### Requests for managing HTTP requests and responses\n",
    "\n",
    "[Requests](https://requests.readthedocs.io/en/master/) also handles other intricacies, including sessions and cookies and URL and form encoding.\n",
    "\n",
    "We can install Requests using pip, Python's default package manager:\n",
    "\n",
    "```sh\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "Then we can import the main module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a `GET` Request\n",
    "\n",
    "The most common HTTP request method is [`GET`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET), which simply gets you a copy of the requested resource located at the URL.\n",
    "\n",
    "\n",
    "Let's make a `GET` request for the [home page](https://www.mshp.dps.missouri.gov/HP68/SearchAction) of the State Highway Patrol website. The convention is to store the response in a variable called `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.mshp.dps.missouri.gov/HP68/SearchAction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`.get`](https://2.python-requests.org/en/master/api/#requests.get) function call has one required argument, which is the URL we want to get. This method returns a [`request.Response`](https://2.python-requests.org/en/master/api/#requests.Response) object that represents the server's response.\n",
    "\n",
    "We can check if we got an \"okay\" response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the specific [response status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the content of the response, that is, all the HTML that makes up the web page. We do this with `.content` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This content is represented in Python in [`bytes`](https://docs.python.org/3/library/stdtypes.html#bytes). If we want the HTML as a string, we can use the response's `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_as_text = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw HTML is a bit overwhelming for human readers because it contains the full definition of every element and the entire hierarchy of the document.\n",
    "\n",
    "Instead, we can write the content to a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.html\", 'w') as f:\n",
    "    f.write(content_as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then open it in a web browser (or in Jupyter Lab), which will display the document without any associated CSS or Javascript files. Hence, we get an unstyled, bare-bones view of the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a `POST` request\n",
    "\n",
    "After `GET`, the second most common HTTP request method is [`POST`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST).\n",
    "\n",
    "A `POST` method allows a user agent to send data to a server, which typically happens via a web form, such as the search form on the Missouri State Highway Patrol's home page.\n",
    "\n",
    "Granted, sending a `POST` method for the purposes of getting search results is a little confusing. Wouldn't a `GET` request make more sense?\n",
    "\n",
    "Ideally, yes. However, search forms are often implemented with a `POST` method request especially if they allow users to submit really complex search queries or require users to submit sensitive data.\n",
    "\n",
    "To figure out which method a search form requires, use your browser's web inspector. You can either check the method attribute of the `<form>` element in the HTML. Or you can watch the network tab for the request and check the method there.\n",
    "\n",
    "The web inspector also reveals other essential information: The names of the form fields. Again, these can be found in either\n",
    "\n",
    "- in the HTML (look at the name attribute on the form's various input fields);\n",
    "- in the header of the `POST` request.\n",
    "\n",
    "We need to know the names of the form fields and what values they will accept because we need to provide this information in our request for search results.\n",
    "\n",
    "In the Python Requests library, this search info is specified via the `data` keyword argument when calling the [`.post()`](https://2.python-requests.org/en/master/api/#requests.post) function.\n",
    "\n",
    "For instance, here's how to search for all the crashes that resulted in at least one fatal injury type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\"https://www.mshp.dps.missouri.gov/HP68/SearchAction\", data={'searchInjury': 'FATAL'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the response is okay, we can write this to another local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup for parsing HTML\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is the most popular Python package for getting data out of HTML.\n",
    "\n",
    "Just like Requests or any other third-party Python package, we can install BeautifulSoup via pip:\n",
    "\n",
    "```sh\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "The parsing process begins by creating a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup) object.\n",
    "\n",
    "First we import this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an instance of this class by passing in the HTML document's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`BeautifulSoup` object](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup) represents the entire parsed HTML document, which we can now [navigate](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree) and [search](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree).\n",
    "\n",
    "Technically, BeautifulSoup is an interface to lower-level Python parsing libraries. By default, it uses the the [`html.parser`](https://docs.python.org/3/library/html.parser.html) module in Python's standard library. But we have other options:\n",
    "\n",
    "- [lxml](https://lxml.de/) tends to be faster\n",
    "- [html5lib](https://html5lib.readthedocs.io/en/latest/) tends to be more lenient (i.e., tolerant of html docs with unclosed tags and other sub-standard syntax)\n",
    "\n",
    "BeautifulSoup's docs have more details about [how to install alternate parsers](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser) and the [differences](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers) between them.\n",
    "\n",
    "If we want to use a different parser, we pass in the name of that parser (as a `str`) in the second positional argument when we create the `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now set up to extract data from the document.\n",
    "\n",
    "At this point, we need to decide exactly what content in this document we want to capture. Web pages of course are designed to be read by humans. They have images, navigation menus, headings, paragraphs and other artifacts of the document's layout, most of which is probably irrelevant to our current reporting task.\n",
    "\n",
    "We need to exercise our [news judgment](https://source.opennews.org/articles/making-good-news-judgments/) to decide exactly what information in this document is important to us. Then we identify patterns for finding that information in the document, and define rules based on those patterns. Finally, we encode that knowledge into Python's syntax.\n",
    "\n",
    "In addition to helping you identify and describe the most relevant HTTP request/responses, your web browser's \"developer tools\" can help you sort through the elements of the web page. This tool is often called \"Inspect element\" or the \"web inspector\", and it allows you to see the full definition of each element, including the tag name and the element's attributes. You can also just use \"view page source\" or download and open the HTML in a text editor.\n",
    "\n",
    "The order and hierarchy of the elements in the HTML document, along with the tag names and the attributes of each element, are useful for finding the exact elements that contain the content we want to extract.\n",
    "\n",
    "#### BeautifulSoup's `.find` method\n",
    "\n",
    "For instance, the results of all of the results of our previous search are contained within an element annotated with a [`<table>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/table) tag, which we can find via BeautifulSoup's [`.find`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find` method returns either a [`Tag` object](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag) or a `NoneType` if there aren't any elements in the document with the given tag name. We can now access the tag's name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['accidentOutput'],\n",
       " 'summary': 'Listing of crash reports based on search criteria.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are provided as a `dict`, allowing us to access each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Listing of crash reports based on search criteria.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.attrs['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup's `.find_all` method\n",
    "\n",
    "BeautifulSoup's `.find` method returns the first tag that meets our specified criteria. However, HTML documents typically have many elements annotated with the same tag name (e.g., a several sub-headings annotated with `<h2>` through `<h6>` tags, several paragraphs of text annotated with `<p>` tags, and sometimes even mulitiple tables of data all annotated with `<table>` tags).\n",
    "\n",
    "BeautifulSoup allows us to find and operate the entire set of tags that have the same criteria using the `.find_all` method.\n",
    "\n",
    "For instance, we can find all of the elements with a `<table>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas `.find` returns an individual `Tag` object, `.find_all` returns a `ResultSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is an iterable (like a list) of `Tag` objects. Because it's an iterable, we can access specific items within a `ResultSet`. Here's how we get the `'summary'` attribute of the first table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Listing of crash reports based on search criteria.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0].attrs['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the length of a `ResultSet`, which is effectively counting the number of `<table>` elements in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `.find` and `.find_all` methods are available on Beautiful's `Tag` object, which allows us to search within the contents of a given element.\n",
    "\n",
    "For instance, let's get all of the column headers of the search results table, each of which is annotated with a [`<th>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/th) (aka, table header) tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_all = table.find_all('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over those tags, and print their text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report\n",
      "Name\n",
      "Age\n",
      "Person City/State\n",
      "Personal Injury\n",
      "Safety Device\n",
      "Date\n",
      "Time\n",
      "Crash County\n",
      "Crash Location\n",
      "Troop\n"
     ]
    }
   ],
   "source": [
    "for th in th_all:\n",
    "    print(th.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These headers will be useful down the road when we write our data to a csv file. We'll keep them around in a `list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which we will append a cleaned up version of each column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in th_all:\n",
    "    header = th.text.strip().replace(\" \", \"_\").replace(\"/\", '_').lower()\n",
    "    headers.append(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['report',\n",
       " 'name',\n",
       " 'age',\n",
       " 'person_city_state',\n",
       " 'personal_injury',\n",
       " 'safety_device',\n",
       " 'date',\n",
       " 'time',\n",
       " 'crash_county',\n",
       " 'crash_location',\n",
       " 'troop']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the actual data—the rows of column values—out of this table.\n",
    "\n",
    "Let's first find all of the table's rows, which are annotated with a [`<tr>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/tr) (aka, table row) tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_all = table.find_all('tr')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we're skipping the first `<tr>` because it contains the table headers we've already extracted.\n",
    "\n",
    "The gist of the process is to grab the text within each [`<td>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/td) (aka, table data cell) element within each `<tr>` element.\n",
    "\n",
    "To better understand how this will work, let's first simply print our expected output for the first few rows.\n",
    "\n",
    "For each row, we'll print a visual delimiter (e.g., a string of dashes). Then for each row, we'll print the text (with whitespace striped) of each cell. We'll print the text of each element on a separate line to demonstrate that we are indeed accessing each element separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "View\n",
      "WORS, RICHARD W\n",
      "42\n",
      "FESTUS, MO\n",
      "FATAL\n",
      "NO\n",
      "11/18/2020\n",
      "4:53PM\n",
      "DOUGLAS\n",
      "PRIVATE PROPERTY EIGHT MILES SOUTHWEST OF WILLOW SPRINGS\n",
      "G\n",
      "--------------\n",
      "View\n",
      "LUTTRULL, JEANA L\n",
      "58\n",
      "LEWISTOWN, MO\n",
      "FATAL\n",
      "YES\n",
      "11/17/2020\n",
      "4:40PM\n",
      "LEWIS\n",
      "MO 6 AT VINE ST IN LEWISTOWN, MO\n",
      "B\n",
      "--------------\n",
      "View\n",
      "LUCKETT, JUSTIN R\n",
      "43\n",
      "ST. CHARLES, MO\n",
      "FATAL\n",
      "YES\n",
      "11/15/2020\n",
      "6:00PM\n",
      "ST. CHARLES\n",
      "EASTBOUND MISSOURI ROUTE 94 .9 OF A MILE EAST OF THE WELDON SPRING BOAT ACCESS\n",
      "C\n",
      "--------------\n",
      "View\n",
      "DAVIS, DENNIS E\n",
      "61\n",
      "AUXVASSE, MO\n",
      "FATAL\n",
      "NO\n",
      "11/15/2020\n",
      "2:30AM\n",
      "CALLAWAY\n",
      "US 54 WESTBOUND WEST OF COUNTRY ROAD 982\n",
      "F\n",
      "--------------\n",
      "View\n",
      "JUVENILE\n",
      "14\n",
      "HILLSBORO, MO\n",
      "FATAL\n",
      "NO\n",
      "11/14/2020\n",
      "4:50PM\n",
      "JEFFERSON\n",
      "SB HIGHWAY B SOUTH OF ROCKY FARM ROAD\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "for tr in tr_all[:5]:\n",
    "    print('--------------')\n",
    "    for td in tr.find_all('td'):\n",
    "        print(td.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first column value for every row is `View`, which seems pretty useless. Look back at the web page or markup, and you'll notice that this text is a hyperlink to another web page with additional details for a given incident report.\n",
    "\n",
    "Knowing that we might want to scrape data from this page as well, it would be helpful to grab the URL of these hyperlinks so that we can make additional requests for this content down the road.\n",
    "\n",
    "In HTML, a hyperlink defined by an [anchor element](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a), which is annotated with an `<a>` tag.  The opening `<a>` tag has an [`href`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a#href) attrribute where the URL is stored.\n",
    "\n",
    "Let's access the value of the `href` attribute for the first row in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/HP68/AccidentDetailsAction?ACC_RPT_NUM=200576955'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_all[0].td.a.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Functions are good way to encapsulate a discrete step a program needs to perform. This approach makes your code easier to read, understand and maintain.\n",
    "\n",
    "For example, we can encapsulate the cleaning process for an entire row in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row(tr):\n",
    "    \n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    row = {\n",
    "        'report': tds[0].find('a').attrs['href'].strip(),\n",
    "        'name': tds[1].text.strip(),\n",
    "        'age': tds[2].text.strip(),\n",
    "        'person_city_state': tds[3].text.strip(),\n",
    "        'personal_injury': tds[4].text.strip(),\n",
    "        'safety_device': tds[5].text.strip(),\n",
    "        'date': tds[6].text.strip(),\n",
    "        'time': tds[7].text.strip(),\n",
    "        'crash_county': tds[8].text.strip(),\n",
    "        'crash_location': tds[9].text.strip(),\n",
    "        'troop': tds[10].text.strip()\n",
    "    }\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop over the first five rows, call this function and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200576955', 'name': 'WORS, RICHARD W', 'age': '42', 'person_city_state': 'FESTUS, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/18/2020', 'time': '4:53PM', 'crash_county': 'DOUGLAS', 'crash_location': 'PRIVATE PROPERTY EIGHT MILES SOUTHWEST OF WILLOW SPRINGS', 'troop': 'G'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200575158', 'name': 'LUTTRULL, JEANA L', 'age': '58', 'person_city_state': 'LEWISTOWN, MO', 'personal_injury': 'FATAL', 'safety_device': 'YES', 'date': '11/17/2020', 'time': '4:40PM', 'crash_county': 'LEWIS', 'crash_location': 'MO 6 AT VINE ST IN LEWISTOWN, MO', 'troop': 'B'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200571996', 'name': 'LUCKETT, JUSTIN R', 'age': '43', 'person_city_state': 'ST. CHARLES, MO', 'personal_injury': 'FATAL', 'safety_device': 'YES', 'date': '11/15/2020', 'time': '6:00PM', 'crash_county': 'ST. CHARLES', 'crash_location': 'EASTBOUND MISSOURI ROUTE 94 .9 OF A MILE EAST OF THE WELDON SPRING BOAT ACCESS', 'troop': 'C'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200570805', 'name': 'DAVIS, DENNIS E', 'age': '61', 'person_city_state': 'AUXVASSE, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/15/2020', 'time': '2:30AM', 'crash_county': 'CALLAWAY', 'crash_location': 'US 54 WESTBOUND WEST OF COUNTRY ROAD 982', 'troop': 'F'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200570053', 'name': 'JUVENILE', 'age': '14', 'person_city_state': 'HILLSBORO, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/14/2020', 'time': '4:50PM', 'crash_county': 'JEFFERSON', 'crash_location': 'SB HIGHWAY B SOUTH OF ROCKY FARM ROAD', 'troop': 'C'}\n"
     ]
    }
   ],
   "source": [
    "for tr in tr_all[:5]:\n",
    "    print('--------------')\n",
    "    row = clean_row(tr)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like clean data! Notice how representing each row as a `dict` helps us quickly assess that we've properly mapped column values to to the proper column headers.\n",
    "\n",
    "We could take this validation further. Each column value in each row is in Python's native `str` data type. We could instead coerce these values to more precise and useful data types (e.g., validate that every `'age'` is an `int`). Python's [`dataclasses`](https://docs.python.org/3/library/dataclasses.html), a rather new feature of the language, would be good for this.\n",
    "\n",
    "Extra work would definitely help us identify all of the inconsistencies in our data and force us to make decisions about how to handle them, likely resulting in more reporting and more conditional logic in our scraper code. Welcome to [yak-shaving](https://softwareengineering.stackexchange.com/questions/388092/what-exactly-is-yak-shaving).\n",
    "\n",
    "Is this the right time to begin enforcing consistency in our data? There isn't a clear cut answer. However, one common practical approach when taking a first cut at a scraper or data extraction process is to take a more *naive*, one that doesn't rely on preconceived notions of how the data should be formed.\n",
    "\n",
    "Below is another version of the row cleaning function we defined above. But this time, the keys are not \"hard-coded\". The only assumptions we are making here is that:\n",
    "\n",
    "- The table cell contains an `<a>` tag with an `href` attribute; and\n",
    "- The order of the table cells matches the order of the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row_alt(tr):\n",
    "    \n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    url = tds[0].find('a').attrs['href'].strip()\n",
    "    \n",
    "    values = [url] + [td.text.strip() for td in tds[1:]]\n",
    "    \n",
    "    return dict(zip(headers, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this function is much more concise. Maybe even a little dense.\n",
    "\n",
    "Now let's print the results of calling this function on the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200576955', 'name': 'WORS, RICHARD W', 'age': '42', 'person_city_state': 'FESTUS, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/18/2020', 'time': '4:53PM', 'crash_county': 'DOUGLAS', 'crash_location': 'PRIVATE PROPERTY EIGHT MILES SOUTHWEST OF WILLOW SPRINGS', 'troop': 'G'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200575158', 'name': 'LUTTRULL, JEANA L', 'age': '58', 'person_city_state': 'LEWISTOWN, MO', 'personal_injury': 'FATAL', 'safety_device': 'YES', 'date': '11/17/2020', 'time': '4:40PM', 'crash_county': 'LEWIS', 'crash_location': 'MO 6 AT VINE ST IN LEWISTOWN, MO', 'troop': 'B'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200571996', 'name': 'LUCKETT, JUSTIN R', 'age': '43', 'person_city_state': 'ST. CHARLES, MO', 'personal_injury': 'FATAL', 'safety_device': 'YES', 'date': '11/15/2020', 'time': '6:00PM', 'crash_county': 'ST. CHARLES', 'crash_location': 'EASTBOUND MISSOURI ROUTE 94 .9 OF A MILE EAST OF THE WELDON SPRING BOAT ACCESS', 'troop': 'C'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200570805', 'name': 'DAVIS, DENNIS E', 'age': '61', 'person_city_state': 'AUXVASSE, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/15/2020', 'time': '2:30AM', 'crash_county': 'CALLAWAY', 'crash_location': 'US 54 WESTBOUND WEST OF COUNTRY ROAD 982', 'troop': 'F'}\n",
      "--------------\n",
      "{'report': '/HP68/AccidentDetailsAction?ACC_RPT_NUM=200570053', 'name': 'JUVENILE', 'age': '14', 'person_city_state': 'HILLSBORO, MO', 'personal_injury': 'FATAL', 'safety_device': 'NO', 'date': '11/14/2020', 'time': '4:50PM', 'crash_county': 'JEFFERSON', 'crash_location': 'SB HIGHWAY B SOUTH OF ROCKY FARM ROAD', 'troop': 'C'}\n"
     ]
    }
   ],
   "source": [
    "for tr in tr_all[:5]:\n",
    "    print('--------------')\n",
    "    row = clean_row_alt(tr)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same output. Awesome! An added benefit to this approach is that, if the web developers make any changes this table element (e.g., re-order the columns or subtract or add new ones), unit of the code will continue to function properly.\n",
    "\n",
    "Poorly timed consistency in our data workflows causes *brittleness* as opposed to fluidness. We do need to enforce (or at the very least check) consistency in our data before we analyze, visualize or otherwise make use of the data. However, in a data pipeline that's extracting data from an inconsistent source, we don't want to enforce consistency to early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the data\n",
    "\n",
    "We've demonstrated how to clean the first few rows. Now let's prepare all of the rows to be stored for later use. First, store all of the rows in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    clean_row_alt(tr) for tr in tr_all\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even check if we got them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `assert` statements are useful for verifying our assumptions. In this case, we are checking to make sure the count of rows we extracted matches the count of `<tr>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rows) == len(tr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [csv](https://docs.python.org/3.8/library/csv.html) module that is part of Python's standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the `DictWriter` class to write out the rows (each being `dict`) to a local csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.csv', 'w', newline='') as f:\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting your notebook to a script\n",
    "\n",
    "Jupyter is a good environment for experimenting and explaining a web scraper. However, if we want to write a program that we can run on a schedule and/or deploy to the cloud, we would probably rather define our web scraper in vanilla Python, either a single script or a module.\n",
    "\n",
    "[nbconvert](https://pypi.org/project/nbconvert/) is a command-line utility for converting iPython notebooks (.ipynb files) into Python scripts (.py files).\n",
    "\n",
    "We can install it with pip:\n",
    "\n",
    "```sh\n",
    "pip install nbconvert\n",
    "```\n",
    "\n",
    "Then invoke it as subcommand of jupyter:\n",
    "\n",
    "```sh\n",
    "jupyter nbconvert --no-prompt --to python walk-through.ipynb\n",
    "```\n",
    "\n",
    "This creates a new file .py file named after the original .ipynb file. Notice also that all of the markdown is also included in the .py file as code comments.\n",
    "\n",
    "For a notebook (such as this one) with more prose than code, we might prefer to exclude the markdown:\n",
    "\n",
    "```sh\n",
    "jupyter nbconvert --no-prompt --to python --PythonExporter.exclude_markdown=True walk-through.ipynb\n",
    "```\n",
    "\n",
    "Now we can open and edit .py in a text editor. If you like, you can just use Jupyter Lab's text editor, but you might prefer a more full-featured text editor (e.g., [Sublime Text](https://www.sublimetext.com/), [Atom](https://atom.io/) or [Visual Code Studio](https://code.visualstudio.com/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legalities of web scraping\n",
    "\n",
    "U.S. law as more blurry lines than bright ones, and this is especially true when it comes to digital technologies. If web scraping is an activity you are going to do regularly, I encourage you to do your own research on this topic.\n",
    "\n",
    "[*Web Scraping with Python, 2nd Edition*](https://learning.oreilly.com/library/view/web-scraping-with/9781491985564/) by Ryan Mitchell has a thorough breakdown of this issue in [\"Chapter 18. The Legalities and Ethics of Web Scraping\"](https://learning.oreilly.com/library/view/web-scraping-with/9781491985564/ch18.html#c-19).\n",
    "\n",
    "Since this book is published by OReilly, you can access it for free through MU Libraries Safari books account. If not, reach out to the j-school library. They are extraordinarily helpful.\n",
    "\n",
    "The last time did my own \"research\" (i.e., Googling) there was a lot of chatter about a 2018 case in the D.C. federal district court, [*Sandvig v. Sessions*](https://www.aclu.org/legal-document/sandvig-v-sessions-opinion), No. 1:16-cv--01368, Dkt. 24 (D.D.C. Mar. 30, 2018). The ruling suggests that at least most of what passes for web scraping would not violate the (rather archaic and vague) federal Computer Fraud and Abuse Act. Here are some takes:\n",
    "\n",
    "- <http://scraping.pro/us-court-scraping-against-tos-legal/>\n",
    "- <https://www.eff.org/deeplinks/2018/04/dc-court-accessing-public-information-not-computer-crime>\n",
    "- <https://www.technologylawdispatch.com/2018/05/big-data/d-c-federal-court-rules-that-web-scraping-does-not-violate-the-cfaa-and-may-be-protected-by-the-first-amendment/>\n",
    "\n",
    "LinkedIn/Microsoft also lost a [similar case](https://arstechnica.com/tech-policy/2017/08/court-rejects-linkedin-claim-that-unauthorized-scraping-is-hacking/) against another company scraping their site."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
